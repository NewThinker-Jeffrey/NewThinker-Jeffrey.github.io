
[TOC]

## DBoW 方法

论文链接（作者个人网站）：
http://doriangalvez.com/papers/GalvezTRO12.pdf

代码：
https://github.com/dorian3d/DBoW2
https://github.com/dorian3d/DLoopDetector/blob/master/include/DLoopDetector/TemplatedLoopDetector.h

###  loop detect 整体流程

字典： descriptor  -->  word， 图片 ---> 词向量
数据库： 存放Keyframe 和 Keypoint （包括 descriptor）等相关信息


1. 从 query image 提取一组特征点 (及其描述符)
2. 查字典将每个特征点描述符对应到字典中的一个word，得到一组 word
3. 每个word计算tf权重(某个word在当前图像中出现次数除以当前图像特征点数，正好反映了不同word在当前图像中出现的比例)
4. 每个word有自己的 idf 权重 (字典建立时固定，以word value的形式存储在字典中，出现越频繁的word权重越低因为越没有区分度)
5. idf 乘以 tf 得到每个word的总权重，总权重就是一个bow向量BowVector
6. 数据库中的每副图像都有自己的bow向量，将新图像的bow与字典中保存的做对比(求相似度，即归一化的score, 有多种计算score的方式)
7. 筛选出 score 高过某个阈值的几副图像作为候选图像
8. 组匹配(computeIslands)，每个候选图像计算"组 (island)" 匹配得分(候选图像及其附近的几帧图像构成一组)，筛选出得分最高的最优组(group or island)
9. 时间一致性检验(updateTemporalWindow和getConsistentEntries): 连续 n (=3?) 帧匹配到数据库中的同一组 keyframe (island) 附近。本次的最优组与前后几次的最优组应该有重叠图像或者时间间距很近。若当前最优组满足时间一致性检验，也从该组中选取得分最高的最优图
10. 结构一致性检验 (isGeometricallyConsistent_DI): 最优图与query image 之间搜索匹配的特征点对（借助 word 缩小搜索范围）并ransac计算基础矩阵，看是否能找到足够多的 inlier 特征点对；

### inverse index 字典的建立: kMean:

时间一致性： 
VINS 新的 Keyframe 添加到 pose_graph 中后，是否需要更新 闭环检测的字典？不需要，字典只在初始时从 "/support_files/brief_k10L6.bin" 文件中加载。


输入大量 descriptors, 用kMean方法聚类成k类;
- kMean的初始化:
 - 首先，任选一个descriptor作为第一个cluster的中心;
 - 假设现在已经得到n个cluster的中心，则计算第n+1个cluster中心的方法如下: 每个descriptor分别计算其到已有的n个cluster中心的距离，选出其中的最小距离，并为该descriptor分配一个权重，权重等于这个最小距离的平方(距离已有的cluster中心越远的descriptor权重越大)，然后按此权重随机抽取另一个descriptor作为第n+1个cluster的中心;
 - 直到凑够k个cluster;

- kMean的迭代:
 - 按照已有的cluster中心，按最小距离原则将所有descriptors重新分组(划分到距离其最近的cluster中);
 - 计算每个cluster的meanValue，作为该cluster的新中心;
 - 重复以上步骤直至每个cluster都不再变化.

然后再对已经分好的k个类，每个类内部再进行kMean分类;
直到分类到第L层，停止。(相当于建立了一个k叉树)
建立这棵树的过程中，每个node都记录下与之对应的cluster的中心descriptor;
叶子节点作为字典的word.


## 结构一致性检验的一些细节

VINS 中的 loop closure 检测，也来自于 [DLoopDetector] (https://github.com/dorian3d/DLoopDetector)，核心文件为```TemplatedLoopDetector.h```，依赖 [DBoW2](https://github.com/dorian3d/DBoW2) 和 [DVision] (https://github.com/dorian3d/DLib/blob/master/src/DVision/FSolver.cpp)。

原版DLoopDetector中，checkFundamentalMat() 时直接使用了keypoint的图像坐标，因此用 Fundamental matrix 来校验几何一致性；但注意这种方法只适用于图像畸变不严重的情况，否则要先做反畸变处理才能保证 Fundamental matrix 的有效性。

VINS 中使用的修改过的 TemplatedLoopDetector.h，自己重新实现了 checkFoundamental()，而且对keypoint坐标先通过 liftProjective() 做了反畸变和 lift 处理 （Lifts a point from the image plane to its projective ray）
```
void EquidistantCamera::liftProjective(const Eigen::Vector2d& p, Eigen::Vector3d& P)
```
然后再用  cv::findFundamentalMat()   来 RANSAC 求基础矩阵（最少一般需要7到8个点可以求基础矩阵）。
VINS 这里的处理做了反畸变，这是一个改进。
但反畸变后，其实没必要再继续求解 基础矩阵了，直接求解 本征矩阵（一般最少5个点可以求本征矩阵，点多了更容易求） 或者 PnP RANSAC （[maplab 做法]( https://github.com/ethz-asl/maplab/wiki/Understanding-loop-closure) ：地图中的 landmark 坐标已给定） 其实更有效。

### 

下面是一些代码细节
参考VINS代码: 
git-sha: f5904b13c0ee649b6c1d4c7f3def8a228cb6dad7:


TemplatedVocabulary::transform() 用于将一副图像query_image(的keypoint/descriptor列表)转化为一个BowVector以便在字典内检索相似图像。
它有两种重载形式，第一种只输出query_image的BowVector, 第二种还可输出另一个更详细的FeatureVector。
通过FeatureVector可以知道query_image中的keypoint匹配到了字典中的哪些node、每个node对应了query_image中的哪几个keypoint.


```c++
//<VINS-Mono/vins_estimator/src/loop-closure/ThirdParty/DBoW/FeatureVector.h>
namespace DBoW2 {
/// Vector of nodes with indexes of local features
class FeatureVector:  // 
  public std::map<NodeId, std::vector<unsigned int> > // 字典中的nodeID到query_image中的特征点索引(列表)的映射; 一副query_image中可能有多个keypoint被映射进同一个node.
  {
  ...
  }
}
```


```c++
//<VINS-Mono/vins_estimator/src/loop-closure/ThirdParty/DBoW/TemplatedVocabulary.h>
namespace DBoW2 {
/// @param TDescriptor class of descriptor
/// @param F class of descriptor functions
template<class TDescriptor, class F>
/// Generic Vocabulary
class TemplatedVocabulary
{        
  ...
  /**
   * Transforms a set of descriptores into a bow vector
   * @param features
   * @param v (out) bow vector of weighted words
   */
  // 将图像(features)转化为BowVector
  virtual void transform(const std::vector<TDescriptor>& features, BowVector &v) 
    const;

  /**
   * Transform a set of descriptors into a bow vector and a feature vector
   * @param features
   * @param v (out) bow vector
   * @param fv (out) feature vector of nodes and feature indexes
   * @param levelsup levels to go up the vocabulary tree to get the node index
   */
  // 将图像(features)转化为BowVector，并且生成FeatureVector。
  // 注意最后一个参数levelsup，它用来控制在生成FeatureVector时，字典中node的检测范围。
  // 确切的说levelsup指定了生成FeatureVector时最多检索到字典树的哪一层(倒数第几层)，
  // 然后FeatureVector中最深只记录这一层的node的Featrue，不会再进一步细分。
  // 如果levelsup设置为0，则检索到最深层的叶节点(word节点)。
  // node所在层越深，该node中包含的Featrue相互之间相似度越高，但包含的Feature数目也越少(因为被细分的次数多)，
  // 在回环检测时，为了寻找两幅图片之间匹配的特征点，需要调整levelsup(比如可以尝试设为2或3)，我们希望FeatureVector中的
  // node所在的层不要太深，这样每个node可以包含尽可能多的相似特征点(尽管他们之间的相似性会降低)，然后我们
  // 在这么多相似特征点里再搜索候选匹配。这样匹配到的概率会提高.
  virtual void transform(const std::vector<TDescriptor>& features,
    BowVector &v, FeatureVector &fv, int levelsup) const;
  ...
}
```

```c++
//<VINS-Mono/vins_estimator/src/loop-closure/TemplatedLoopDetector.h>
template<class TDescriptor, class F>
bool TemplatedLoopDetector<TDescriptor, F>::isGeometricallyConsistent_DI(
  EntryId old_entry, const std::vector<cv::KeyPoint> &keys, 
  const std::vector<TDescriptor> &descriptors, 
  const FeatureVector &bowvec, // query_image的FeatureVector
  std::vector<cv::Point2f> &cur_pts,
  std::vector<cv::Point2f> &old_pts)
{
  const FeatureVector &oldvec = m_database->retrieveFeatures(old_entry); // loop_image的FeatureVector

  // for each word in common, get the closest descriptors

  vector<unsigned int> i_old, i_cur; // 这两个数组将用来记录query_image(cur_image) 与 loop_image(old_image)之间的候选匹配特征点在各自图像中的索引

  FeatureVector::const_iterator old_it, cur_it; 
  const FeatureVector::const_iterator old_end = oldvec.end();
  const FeatureVector::const_iterator cur_end = bowvec.end();

  old_it = oldvec.begin();
  cur_it = bowvec.begin();

  while(old_it != old_end && cur_it != cur_end)
  {
    if(old_it->first == cur_it->first) // 如果在新旧图像中发现了属于同一node的特征点(注意每副图像中属于同一node的特征点可能有多个,也就是说我们将得到两组特征点，一组属于新图像，一组属于旧图像)
    {
      // compute matches between 
      // features old_it->second of m_image_keys[old_entry] and
      // features cur_it->second of keys
      vector<unsigned int> i_old_now, i_cur_now;

      //  在这两组特征点中找出匹配度最高的两个(每组中各一个)。
      // 如果这两个特征点记作i_old_now和i_cur_now，匹配度是指：从两组中各任选一个特征点时，它们间的匹配度一定不大于a，b间的匹配度
      getMatches_neighratio(
        m_image_descriptors[old_entry], old_it->second, 
        descriptors, cur_it->second,  
        i_old_now, i_cur_now);

      i_old.insert(i_old.end(), i_old_now.begin(), i_old_now.end()); // 记录选出的两个特征点，这两个点将作为一组匹配，在后续过程中ransac计算里两幅图像间的基础矩阵
      i_cur.insert(i_cur.end(), i_cur_now.begin(), i_cur_now.end());

      // move old_it and cur_it forward
      ++old_it;
      ++cur_it;
    }
    else if(old_it->first < cur_it->first)
    {
      // move old_it forward
      old_it = oldvec.lower_bound(cur_it->first);
      // old_it = (first element >= cur_it.id)
    }
    else
    {
      // move cur_it forward
      cur_it = bowvec.lower_bound(old_it->first);
      // cur_it = (first element >= old_it.id)
    }
  }

  // ransac计算基础矩阵
  // calculate now the fundamental matrix
  if((int)i_old.size() >= m_params.min_Fpoints)
  {
    vector<cv::Point2f> old_points, cur_points;

    // add matches to the vectors to calculate the fundamental matrix
    vector<unsigned int>::const_iterator oit, cit;
    oit = i_old.begin();
    cit = i_cur.begin();

    for(; oit != i_old.end(); ++oit, ++cit)
    {
      const cv::KeyPoint &old_k = m_image_keys[old_entry][*oit];
      const cv::KeyPoint &cur_k = keys[*cit];

      old_points.push_back(old_k.pt);
      cur_points.push_back(cur_k.pt);
    }

    cv::Mat oldMat(old_points.size(), 2, CV_32F, &old_points[0]);
    cv::Mat curMat(cur_points.size(), 2, CV_32F, &cur_points[0]);
    //printf("-old size %d current size %d\n",old_points.size(),cur_points.size());
    bool find_fundamental_suss = false;
    find_fundamental_suss = checkFoundamental(cur_points, old_points, cur_pts, old_pts);
    if(find_fundamental_suss)
    {
      //cur_pts = cur_points;
      //old_pts = old_points;
      return true;
    }
  }

  return false;
} 
```