
[TOC]

## 数值稳定性：矩阵的条件数

https://blogs.mathworks.com/cleve/2017/07/17/what-is-the-condition-number-of-a-matrix/
https://en.wikipedia.org/wiki/Condition_number#Matrices

### 条件数简介

求解线性方程组时，我们通常会谈及到矩阵的"条件数"（condition number）的概念。宽泛地讲，条件数用于衡量一个计算任务（可以抽象为一个函数）的**输出对输入扰动的敏感程度**，这个计算任务甚至可以是非线性的。
事实上，"矩阵$A$的条件数" 本身并不是一个足够清晰的概念：面对不同的计算任务时，$A$可以定义不同的条件数；比如，一个矩阵$A$，对于求特征值和求逆矩阵这两个不同的任务，它可以有不同的条件数，且即便它对求解特征值有很好的条件数，它对求逆运算的条件数也可能很差（A matrix can be poorly conditioned for inversion while the eigenvalue problem is well conditioned ）；
只不过，多数情况下，我们提到"矩阵$A$的条件数"时，暗指的是在求解 $Ax=b$ 这样的线性方程组时的条件数。同样的条件数适用于对矩阵$A$的求逆运算。

### 线性方程组求解问题中的条件数

前面提到，"条件数" 用于衡量一个计算任务的输出对输入扰动的敏感程度。
对于求解线性方程组 $Ax=b$ 这样一个计算任务，给定系数矩阵$A$后，它的输入为$b$，输出为$x$；
因此，我们可以定义矩阵 $A$ （在解方程组这个任务中）的条件数$\kappa (A)$为： 解算结果 $x$  的相对误差 $\frac{|\delta x|}{|x|}$ 与 $b$ 的相对误差 $\frac{|\delta b|}{|b|}$ 之比的上限，即
 $$\kappa (A)=\max \left(\frac{|\delta x|}{|x|}/\frac{|\delta b|}{|b|}\right)\tag{1}$$
注意，这里涉及到了 $\delta b,b, \delta x, x$ 这四个向量的范数($|\delta b|,|b|, |\delta x|, |x|$)。可见，即便对于解方程这个特定的计算任务，$A$的条件数的定义也不唯一：它取决于我们选择什么样的向量范数。
下面，我们详细展开这一常见的矩阵条件数的定义。
$A$ 做一个非退化的矩阵（对应一个非退化的线性变换），它的诱导范数定义为 
$$\Vert A \Vert = \max \frac{| Ax |}{|x|}$$
我们令 $M=\Vert A \Vert$, 则 $M$ 实际上衡量了$A$作用于向量时，对向量的最大"拉伸"程度；
相似地，我们可以定义 $A$ 对向量的最小"拉伸"（或最大 "压缩"）程度为 
$$m=\min \frac{| Ax |}{|x|} = \min \frac{| b |}{|A^{-1}b|} = \frac{1}{\max \frac{|A^{-1}b|}{| b |} } = \frac{1}{\Vert A^{-1}\Vert}$$
矩阵$A$的最大拉伸作用 $M$ 与最小拉伸作用 $m$ 的比值 ，就是上面 (1) 式的一个等价定义：
$$\kappa (A)= \frac{M}{m}=\Vert A \Vert \Vert A^{-1} \Vert \tag{2}$$
注意根据矩阵范数与矩阵乘法的相容性， 我们始终有 $\kappa (A) = \Vert A \Vert \Vert A^{-1} \Vert \ge \Vert A  A^{-1} \Vert = 1$，即条件数不会小于1。

> (1)与(2)等价的证明：
$ A\delta x = \delta b \Rightarrow |\delta b|\ge m|\delta x| \quad\quad \text{(依据m的定义)}$
$ Ax = b \Rightarrow |b|\le M|x| \quad\quad \text{(依据M的定义)}$
故 $\frac{m|\delta x|}{M|x|} \le \frac{|\delta b|}{|b|}$，即
$\frac{|\delta x|}{|x|}/\frac{|\delta b|}{|b|} \le (M/m) $，
因此 $(M/m)$ 是 $\frac{|\delta x|}{|x|}/\frac{|\delta b|}{|b|}$ 的一个上界；我们接下来还要说明它是上确界。
根据$m$和$M$的定义，存在$x$和$\delta b$  使得 $|Ax|=M|x|$ 且 $|A\delta b|=m|b|$，而这时，上面不等式就可以取到等号。
因此，
$$ \max \left(\frac{|\delta x|}{|x|}/\frac{|\delta b|}{|b|}\right) =   \frac{M}{m}=\Vert A \Vert \Vert A^{-1} \Vert $$


### 矩阵条件数的具体形式

如果**向量范数选择欧氏范数（2-范数）**，容易证明，
$$\kappa (A)= \Vert A \Vert \Vert A^{-1} \Vert = \frac{\sigma_{max}(A)}{\sigma_{min}(A)}$$
其中，${\sigma_{max}(A)},{\sigma_{min}(A)}$ 分别矩阵 $A$ 的最大、最小奇异值。实际工程中，尤其在$A$是维数很高的矩阵时，精确计算2-范数下的条件数几乎不可行（因为要计算奇异值），只能对其做一些量级估计。

> 证明：
考虑$A$的SVD分解 $A=UDV^T$，在2-范数下，我们有 $|Ax|=|UDV^Tx|=|Dx|$ （这里用到了$U,V$都是正交阵的特性，正交阵作用于向量时不改变向量在2-范数下的长度）。而
$\frac {|Dx|}{|x|}=\sqrt{\frac{\sum (\sigma_ix_i)^2} {\sum  x_i^2}} \le  \sqrt{\frac{\sum (\sigma_{max} x_i)^2} {\sum  x_i^2}} = \sigma_{max}$
即 $\frac {|Ax|}{|x|}=\frac {|Dx|}{|x|} \le \sigma_{max}$，且等号可以取到（当 $x$ 只在 $\sigma_{max}$ 对应的维度方向上有非0分量时），进而有 $\Vert A \Vert=\sigma_{max}$。注意，矩阵的奇异值都是非负的，所以$ \sigma_{max}$ 不用加绝对值符号。
同理，$\Vert A^{-1} \Vert=\frac{1}{\sigma_{min}}$（注意到$A^{-1}$的最大奇异值是$A$的最小奇异值之倒数）。
因此有 $\kappa (A)= \Vert A \Vert \Vert A^{-1} \Vert = \frac{\sigma_{max}(A)}{\sigma_{min}(A)}$


重要特例：
- 如果$A$是 normal matrix (比如实对称矩阵)，$\kappa (A)$ 等于$A$的模长最大、最小**特征值**的模长之比；
- 如果$A$是酉矩阵，比如单位阵，则$\kappa (A) = 1$


## LU 分解法

LU 分解法求解线性方程组本质上就是高斯消元法；

假设 $A$ 是满秩的方阵，我们用 $A=LU$ 代表 $A$ 的 LU 分解，其中$L,R$分别是下三角矩阵和上三角矩阵。
LU分解本质是对矩阵 $A$ 做基础行变换进行高斯消元，把A变成上三角阵。所有基础行变换的作用组合在一起，等价于左乘一个下三角矩阵$L^{-1}$  (注意下三角阵的逆依然是下三角)，消元后剩下的上三角矩阵就是$R$。
对线性方程组右侧的$b$做相同的基础行变换后得到$b'=L^{-1}b$，然后 back substitution回代求解 $Rx=b'$即可得到原问题的解。

> 关于LU分解的唯一性：
- 不对LU做额外要求的话，LU分解不唯一（比如对高斯消元后的上三角矩阵R的每行再乘一个非零系数，就可以得到一个不同的上三角阵R'）；
- 对分解做某些特定限制后，比如要求$L$的对角线都是1、或者$R$的对角线都是1等等，LU分解才唯一；

## QR 分解法

todo.

假设 $A$ 是满秩的方阵，

QR分解的方法：
施密特正交化法：如果两个列向量本来就接近平行，那正交化这两个向量时可能导致数值稳定性问题 ；比如，若$a,b$ 是两个接近平行的向量，正交化时，$b_{\perp}=b-b_{\parallel}=b-\frac{b\cdot a}{|a|^2}a$，这样得到的 $b_{\perp}$ 的模长是非常小的量，计算与其同向的单位向量时就容易受浮点误差影响；

HouseHoder方法：wiki

pivoting: 保持稀疏结构


对 $A_{m\times n}$, $m\ge n$, 列满秩，可以进行QR factorization 或 QR decomposition;
Sparse QR，用施密特正交化法（存在数值稳定性问题），或者用 Householder reflection matrix;
等价于因子图消元。

- todo: 了解 Multifrontal QR ??? 多波前QR？

对于超定方程，QR 分解还可以求解最小二乘解；

## Cholesky 分解法：对称的系数矩阵
 
假设 $A$ 是满秩的对称方阵；

LDLT
pivoting

## 共轭梯度法：对称且正/负定的系数矩阵

我们主要讨论系数矩阵正定的情况。
如果系数矩阵$A$负定，那 $-A$ 就正定，把方程组两边同乘 $-1$，就转化回了系数矩阵正定的情况。

### 思路简介

共轭梯度法求解 $Ax=b$ 或 $Ax-b=0$
使用条件：$A$是对称正定阵
考虑二次函数 $f(x)=\frac{1}{2}x^TAx - b^Tx$，其一阶导为$f'(x)=Ax-b$。
求解 $Ax-b=0=f'(x)$ 等价于求解 $\arg \min_x f(x)$ 。这是一个二次优化问题，可以用优化的方法来求解。

为理解共轭梯度法，首先要理解共轭向量的概念：
共轭向量：向量$p_i$与$p_j$是共轭的，如果$p_i^TAp_j=0$；
- 一组两两共轭的向量$\{p_i\}$，必然是线性无关的，否则与$A$的正定性矛盾。比如，若$\alpha p_1+ \beta p_2= p_3$，那么由$A$的正定性有$p_3^TAp_3>0$，但另一方面由$\{p_i\}$两两之间的共轭性，有 $p_3^TAp_3 = \alpha p_1^TAp_3 + \beta p_2^TAp_3 = 0$，矛盾；
- 一组两两共轭的向量$\{p_i\}$，最多包含$n$个向量（因为它们必须线性无关）；如果$\{p_i\}$包含$n$个共轭向量则称其为一个极大共轭向量组；

由于$A$的对称和正定性，二次函数$f(x)$的"等高面"是"椭球面"；椭球的球心即待求的解；
任意给定初始点$x_0$，该点的梯度方向 (垂直于所在椭球面) 为 $f'(x_0)=Ax_0-b$，hessian阵为 $A$；
从$x_0$出发，为了到达球心（球心梯度为0），就要找一个前进方向抵消掉$x_0$处的梯度$f'(x_0)$，因此$x_0$处的最佳前进向量 $p$ 要满足 $Ap=-f'(x_0)=-(Ax_0-b)$，即 $p=A^{-1}b-x_0$；
可见想直接得到最佳前进向量 $p$，需要计算$A^{-1}$，而这正是我们想要避免的操作；
那我们就要想办法，能否"间接"地到达球心？
共轭方向法提供了一种思路：给定一个极大共轭向量组$\{p_i\}$，从$x_0$出发，依次沿各个共轭方向走到该方向的极小点，注意这个极小点相切于某个等高椭球面。这样，最多n步，就能到达球心（如果通过一个线性变换把解空间映射为一个新的线性空间，在新的线性空间上所有等高面变为球面，那这组共轭向量就变成了新空间的一组正交基或坐标轴，依次沿这些向量找切点相当于依次沿这组坐标轴走到原点，因为只有n个轴，所以最多走n步就能到原点）。

对于二次函数$f(x)$，沿各方向找极小点（切点）可转化为找一个一元二次函数的极小点，可直接解析地算出该点，不需要 line search和迭代。
> 共轭梯度法用于求解一般的最优化问题时（非线性），每一步的前进步长 $\alpha_k$ 是需要 line search 的。实际中可选 exact line search  或 soft  line search；




共轭梯度法是一种具体的共轭方向法。依据初始点 $x_0=0$ 的梯度，以及后续各切点$x_k$ 的梯度和前一步的前进方向的线性组合，来确定下一步的前进方向，使得第$k+1$步的前进方向与前$k$步的前进方向保持共轭。
具体公式不再写出。见 [Chapter_15__Conjugate_Gradient](http://www.math.iit.edu/~fass/477577_Chapter_15.pdf) 15.2节，附件有备份；


共轭梯度法的好处在于: 每步迭代的最大的计算量仅来源于矩阵向量乘法, 并且不需要对系数矩阵进行修改（做矩阵分解或消元）， 因此不会引入 fill-in 的问题. 


### 收敛速度，矩阵$A$的条件数的影响

这里主要从几何角度做定性分析。
理论上，对于$n$维的解空间，用共轭梯度法求解二次函数的最小值，最多需要迭代$n$步。对于$n$特别大的情况，我们不希望算法走完所有这$n$步。
我们知道，共轭梯度法在初始点$x_0=0$处的搜索方向$p_0$是其负梯度方向$p_0=-f'(x_0)$；$x_k$ 是子空间 $span(f'(x_0), f'(x_1) ... f'(x_{k-1}))$ 上的最小值点，在$x_k$处的搜索方向$p_k\in span(f'(x_0), f'(x_1) ... f'(x_k))$；
- 最理想的情况下，如果$A$是单位阵（这时 $\kappa(A) = \kappa(I) = 1$）， 二次函数 $f(x)$ 的"等高面" 是 "球面"，$x_0$的负梯度方向$f'(x_0)$将直接指向"球心"$c$，因此只需1次迭代，我们就能到达$f(x)$的最小值点（即球心$c$）；
- 如果$A$不是单位阵，但它的条件数比较小（$A$的各特征值相差不大），$f(x)$ 的"等高面" 也是近似 "球面"，各点的负梯度方向也都是指向球心$c$附近，而每一步迭代的方向 $p_k$ 也接近$x_k$处的负梯度方向（指向球心附近），所以收敛就比较快，可能只需几步就能收敛到距离解很近的位置；
- 而反过来，如果$A$的条件数很大，那 $f(x)$ 的"等高面" 就是非常椭的 "椭球面"（想象一个飞盘的表面，并极限压扁），这个"椭球面"上大部分点的负梯度方向（垂直于椭球面的方向），与该点到球心的连线方向夹角都很大，甚至可能是接近垂直的！这种情况下迭代过程中，$x_k$刚开始会来回小幅度震荡（接近静止不动），且 $p_k$ 与 $x_k$到球心的方向$(c-x_k)$ 接近正交 （因为 $p_k\in span(f'(x_0), f'(x_1) ... f'(x_k))$，而$f'(x_i)$都与$(c-x_k)$接近正交），靠近球心的速度非常缓慢，导致收敛极慢。

> 下式定量地描述了收敛速率的下限：
$\left\| x_k - x^* \right\|_A \leq 2\left\| x_0 - x^* \right\|_A(\frac{\sqrt{\kappa (a)} - 1}{\sqrt{\kappa (a)} + 1})^{k} $
$\kappa (a)$ 是椭球面的最长轴与最短轴之比；
可见共轭梯度法保证至少 $(\frac{\sqrt{\kappa (a)} - 1}{\sqrt{\kappa (a)} + 1})$ 倍的线性收敛速率。

### PCG: Preconditioned CG

改善条件数可以加快收敛速度。
precondition的操作，是选择一个合适的矩阵$M$，把方程组$Ax=b$ 变成 $M^{-1}Ax=M^{-1}b$，且$M^{-1}A$ 有比较好的条件数；
一个最简单的 preconditioner 是 diagonal 或 Jacobi preconditioner。即选取$A$的diagonal或block diagonal作为$M$。
这个preconditioner不用求大矩阵的逆，但依然要用到 "实数的逆(即倒数)"或"小矩阵块的逆"。各个小矩阵块自身的条件数一般远小于整个大矩阵的条件数， 对小矩阵块单独求逆一般在数值上也是相对稳定的。
另外，PCG这里加 preconditioner 的主要目的，不是为了增加数值稳定性，而是为了增加收敛效率。


## 下一篇todo: 特征值与特征向量求解、SVD分解($A=UDV^T$)

对角阵的特征值是对角线上的值，特征向量是所有的基向量；
QR分解求特征值，对于对称阵通过QR分解还可直接得到特征向量，非对称阵需要进一步用反幂法；



