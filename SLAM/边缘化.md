[TOC]



## MLE/LSE and Marginalization 最大似然/最小二乘估计，与边缘化



ref: http://blog.csdn.net/heyijia0327/article/details/52822104



---------



### General MLE



Let's define some symbols first to model the problem:

- $S$: the state space. a manifold.

- $M\equiv \mathbb R^m$: the measurement space (observation space). an Euclidean space.

- $s^*\in S$: the real (unobservable) state. it should be considered as an unknown constant, rather than a random point.

- $s \in S$: the state variable, an arbitrary guess for $s^*$
- $f: S\rightarrow M$: the observation function.

- $n_y$: the observation noise, a random vector in $\mathbb R^m$,  $n_y\propto \mathcal N (0, \Sigma)$ , for simplicity we choose $\Sigma=I$.

- $y$: the observation (before the experiment), a random vector in $\mathbb R^m$ depending on $n_y$: $y=f(s^*) + n_y$. $y\propto \mathcal N [f(s^*) , \Sigma=I]$ 

- $y^*$: the realized observation, a constant, $y^* \in M\equiv \mathbb R^m$,

- $e_S: S\rightarrow \mathbb R^m$: the error function, a short hand for $f()-y^*$. To be more specific, $e_S(s) = f(s) - y^*$ 

> An alternative way to define $e_S$ is: $e_S(s) =  y^* - f(s)$. But this gives the jacobians of $J_{e_S}$ and $J_{f}$  different signs, i.e. $J_{e_S}=-J_{f}$, which might cause some inconvenience in symbols.


According to the model, the probability (before the experiment) that $y$ happens to be $y^*$ is:

$p(y=y^*) = p[n_y=y^*-f(x^*)=-e_S(s^*)] =  \mathcal N[-e_S(s^*)]\propto \exp[-\frac{1}{2}e_S(s^*) ^Te_S(s^*) ]$



Note this probability is unknown since $s^*$ is always unknown.



Then, the likelyhood of the guess $s$ is defined as (the probability that we get a sample $y^*$ when $s$ is exactly the real state $s^*$"):

$L(s) = p(y=y^*)|_{\text {if } s^*=s} \propto \exp[-\frac{1}{2}e_S(s) ^Te_S(s) ]$.

> $e_S(s)$ depends on the realized observation $y^*$.



The MLE method is to choose an $s$ that maximizes $L(s)$.


---------



### Linearization



In the real world application, $e_S$ is usually nonlinear. To simplify the problem we need linearization.

Let's do some variable substitutions for symbol convenience.

- $s_0\in S$: the linearization point

- $T_{s_0}S$: the tangent space of S at $s_0$

- $x^*\in T_{s_0}S$: the  (unobservable)  linearized state $x^*=s^*\boxminus s_0$, (in other words $s^*=s_0\boxplus x^*$); Just as $s^*$, it should be considered as an unknown constant, rather than a random vector.

- $x\in T_{s_0}S$: the linearized state variable, an arbitrary guess for $x^*$, and $s=s_0\boxplus x$.

- $e_0=e_S(s_0)$: the error (depending on $y^*$) at the linearization point $s_0$

- $e: T_{s_0}S\rightarrow \mathbb R^m$: the error function (depending on $y^*$) against the linearized states. $e(x)=e_S(s_0\boxplus x)=e_S(s)$



By linearizing the $e_S(s)$ around $s_0$ (and $e(x)$ around $0$), we have:  

$ e(x) = e_S(s) \approx e_S(s_0)+J(s\boxminus s_0) = e_0 + Jx$.

As we can see after linearization, the nonlinear functions $e, e_S, f$ all become affine functions.



Then the likelyhood of that "the guess $x$ is exactly the real linearized-state $x^*$"  is defined as:

$$L(x)=p(y=y^*)|_{\text {if } x^*=x}\propto \\
\exp[-\frac{1}{2}e(x) ^Te(x)] \approx \\
\exp[-\frac{1}{2}(e_0^Te_0+2e_0^TJx + x^TJ^TJx)]  \propto \\
\exp\{-\frac{1}{2}[(x-\mu_{y^*})^TH(x-\mu_{y^*})]\}$$.

where $H=J^TJ$, (let $g=J^Te_0$) and $\mu_{y^*}=-H^{-1}g$. Note we require $H$ is invertible here.

>  Note $\mu_{y^*}$ depends on the realized observation $y^*$ since $e_0$ does: $y^*$: $e_0= f(x_0) - y^*$



#### Linearized MLE and LSE (least-squares estimates)



As stated above, the MLE method is to choose an $x$ that maximizes $L(x)$. Let us use $x^{E*}$ to denote such an $x$.

> It's equivalent to minimize the cost function (the exponent of $L(x)$) defined below: (So it's a LSE problem now)

$ F(x) = \frac{1}{2}e(x)^Te(x) \approx \frac{1}{2}(e_0^Te_0+2e_0^TJx + x^TJ^TJx)$

> the gradient and hession of $F(x)$ is:

$F'(x) = e_0^TJ + x^TJ^TJ = (e_0^T + x^TJ^T)J = e(x)^TJ$

$F''(x)^T = J^TJ$

> Note that $F'(0)=g^T, F''=H$. we need to solve $x$ from $Hx=-g$.




It's easy to find that $\mu_{y^*}$ is exactly the expected $x^{E*}$: 

$x^{E*}=\mu_{y^*}=-H^{-1}(J^Te_0)=-H^{-1}J^T[e_S(s_0)]=-H^{-1}J^T[f(s_0)-y^*]$.

Let $x^E(y)$ denote the MLE result for $y^*=y$, i.e. 

$$x^E(y)=\mu_{y}=-H^{-1}J^T[f(s_0)-y] = \\
-H^{-1}J^T[f(s_0)- f(s^*) - n_y] =\\
-H^{-1}J^T[J(s_0\boxminus s^*) - n_y]=\\
-H^{-1}J^T[J(-x^*) - n_y]=\\
H^{-1}Hx^* + H^{-1}J^Tn_y=\\
x^*+H^{-1}J^Tn_y$$

So, before experiment, $x^E$  (the result of our "future" MLE) can be regarded as a random vector depending on the noise $n_y$. 

> 这里要仔细理解一下，$x^E$ 并不是指你实际实验（ 执行观测并计算MLE ）得到的那个结果，而是指你在"获知实验结果前"对实验结果的"推测"，这个"推测"本身是个随机变量；实际实验得到的那个结果，我们用$x^{E*}$表示，这是一个常量。



$x^E$'s covariance is $Cov[x^E]=(H^{-1}J^T)(H^{-1}J^T)^T=H^{-1}J^TJH^{-1}=H^{-1}HH^{-1}=H^{-1}=(J^TJ)^{-1}$, and its expectation $E[x^E]$is exactly the real (linearized) state $x^*$, which means MLE for expection (with the linear and Gaussian condition) is unbiased. 

> b乎上一个值得注意的问题： 在什么条件下，最大似然估计值是无偏估计？

https://www.zhihu.com/question/35670078



Let's write down the PDF of $x^E$:
$x^E\propto \mathcal N[\mu=x^*, (J^TJ)^{-1}]$

$p(x^E)\propto \exp\{-\frac{1}{2}[(x^E-\mu)^TH(x-\mu)]\}$



> The error to our estimation, $(x^E-x^*)$, is also a random vector and  $(x^E-x^*)\propto \mathcal N[0, (J^TJ)^{-1}]$.

> Note the covariance $(J^TJ)^{-1}$ is almost independent on the real observation $y^*$, but only depends on the model itself. We say "almost" since  in practice different choices of reasonable (i.e. near the real state) linearization point $s_0$ also make slight difference to the covariance and the choice procedure might rely on some observations. If  the model itself is nearly linear or we already get a reasonable linearization point $s_0$ somehow (before experiment), then we can immediately know the covariance of the error to our "future" estimation (even if we have not gotten an observation $y^*$ yet!).



#### An interesting viewpoint



The whole process of realizing an observation $y^*$ and then doing MLE (for $x^*$) with it can be considered as getting a sample for the random vector $x^E$, and $x^{E*}$ is the sample. If we use this sample to estimate (also by MLE) $x^E$'s expectation $\mu=x^*$, then we get $x^{E*}$ again.



If the state space $S$ is Euclidean and the observation function is strictly affine (so we can set $x_0=0$ without loss of generality, and $T_{x_0}S=S$ which implies $\boxplus$ is simply "$+$", $s=x+s_0=x$, $s^*=x^*+s_0=x^*$), and the noise $n_y$ is Gaussian, then the following two estimation problems are equivalent:

1. Given the observation variable $y=f(s^*)+n_y=f(x^*)+n_y$ and a realized sample $y^*$; Then we use the sample $y^*$ to estimate (by MLE) $x^*$; 

2. The random vector $x^E$ can be viewed as a pseudo-observation, and $x^{E*}$ is a sample or realization of $x^E$; Then we use the sample $x^{E*}$ to estimate (also by MLE) $x^E$'s expectation $\mu=x^*$



The likelyhood function of 1 is: $L_1(x)=p(y=y^*)|_{\text {if } x^*=x}\propto \exp\{-\frac{1}{2}[(x-\mu_{y^*})^TH(x-\mu_{y^*})]\}$;

The likelyhood function of 2 is: $L_2(x)=p(x^E=x^{E*})|_{\text {if } \mu=x}\propto \exp\{-\frac{1}{2}[(x^{E*}-x)^TH(x^{E*}-x)]\}$;
$L_1(x)=L_2(x)$ since $\mu_{y^*}=x^{E*}$, i.e. the two problem give the save likelyhood function.



If $y^*$ is just part of the observation of another larger problem, then we can remove this part from the larger problem and insert back the pseudo-observation part $x^{E*}$. Note $x^{E*}$ should be the MLE result for the the real states by only considering the $y^*$ part of the total observation; Bacause $y^*$ and $x^{E*}$ will give the same partial-likelyhood. 

> 在实际工程应用中，marginalization 就是在干这个，把一部分观测"扔掉"，但留下一个 marginalization factor 来为与这些观测相关的状态量提供一个先验约束；

从这里的分析也可以看出，虽然扔掉了一部分观测$y^*$，但它们的"作用"(即它们所提供的 partial likelyhood)，被用它们做MLE产生的 pseudo-observation $x^{E*}$ 完全保留了下来，我们并未丢失任何有用的信息；

这是基于观测函数 $f$ 是仿射函数且噪声是高斯的这些假设，如果这些假设不成立，模型本身的非线性、以及边缘化时选取的线性化点与真值的偏差等，都会带来一部分无法弥补的误差；





### Schur complement

Let $x^E=\left[\begin{matrix} x^E_a \\x^E_b \end{matrix}\right]=\left[\begin{matrix} a \\b \end{matrix}\right]$.
We need to solve the equation: $Hx=-g$.
Let's expand the matrices first:


$$\left[\begin{matrix} H_{aa} & H_{ab} \\
H_{ba} & H_{bb} \\
\end{matrix}\right] \left[\begin{matrix} a  \\
 b \\
\end{matrix}\right] =
-\left[\begin{matrix} g_{a}  \\
g_{b} \\
\end{matrix}\right]$$



$$\left[\begin{matrix} H_{aa} & H_{ab} \\
H_{ba} & H_{bb} \\
\end{matrix}\right] \left[\begin{matrix} a  \\
b \\
\end{matrix}\right] =\\
-\left[\begin{matrix} g_{a}  \\
g_{b} \\
\end{matrix}\right]$$



Schur complement : multipy first row-section by $-H_{ba}H^{-1}_{aa}$ from left and add the result to the second  row-section 

> 从解方程的角度看，Schur complement 纯粹就是消元法而已


$$\left[\begin{matrix} H_{aa} & H_{ab} \\
0 & H_{bb}-H_{ba}H^{-1}_{aa}H_{ab}  \\
\end{matrix}\right] \left[\begin{matrix} a  \\
b \\
\end{matrix}\right] = -\left[\begin{matrix} g_{a}  \\
g_{b}-H_{ba}H^{-1}_{aa}g_{a} \\
\end{matrix}\right]$$



$H_{bb}-H_{ba}H^{-1}_{aa}H_{ab}$ is the Schur complement of $H_{aa}$. See [Schur complement](https://en.wikipedia.org/wiki/Schur_complement)



> 根据以上方程，可以"解耦$x_a$"而直接用下式求解$x_b$:  

$(H_{bb}-H_{ba}H^{-1}_{aa}H_{ab})b=-(g_{b}-H_{ba}H^{-1}_{aa}g_{a})$.

$b=-(H_{bb}-H_{ba}H^{-1}_{aa}H_{ab})^{-1} (g_{b}-H_{ba}H^{-1}_{aa}g_{a})$

当$H^{-1}_{aa}$结构比较好、容易计算时（比如若$H^{-1}_{aa}$很稀疏且非0元素都紧挨着对角线），用这种方法先解出$x_b$再代回求解$x_a$可以加速方程的求解。

或者如果只希望用$x_b$时，可以用这种方法跳过对$x_a$的求解；



#### two useful matrix formula

1. 

$$\left[\begin{matrix} H_{aa} & H_{ab} \\
H_{ba} & H_{bb} \\
\end{matrix}\right]^{-1} =  \left[\begin{matrix}I&0\\-H_{bb}^{-1}H_{ba}&I\end{matrix}\right] \left[\begin{matrix}(H_{aa}-H_{ab}H_{bb}^{-1}H_{ba})^{-1}&0\\0&H_{bb}^{-1}\end{matrix}\right]   \left[\begin{matrix}I&-H_{ab}H_{bb}^{-1}\\0&I\end{matrix}\right] $$



2.

$$\left[\begin{matrix} H_{aa} & H_{ab} \\
H_{ba} & H_{bb} \\
\end{matrix}\right]= \left[\begin{matrix}I&H_{ab}H_{bb}^{-1}\\0&I\end{matrix}\right]  \left[\begin{matrix}(H_{aa}-H_{ab}H_{bb}^{-1}H_{ba})&0\\0&H_{bb}\end{matrix}\right]   \left[\begin{matrix}I&0\\H_{bb}^{-1}H_{ba}&I\end{matrix}\right]$$



Consider the symmetry betweens subscripts $a,b$, we can get another two equations by exchanging $a,b$ in 1,2.







### Marginalization



#### Marginal distribution



We'll interprete marginalization from the perspective of probabilty.

And we already know, as a random vector (in $T_{s_0}S$), $x^E\propto \mathcal N[x^*, (J^TJ)^{-1}]$.

Let $x^E=\left[\begin{matrix} x^E_a \\x^E_b \end{matrix}\right]=\left[\begin{matrix} a \\b \end{matrix}\right]$.



$x^E$'s  covariance is $Cov(x^E) = P = H^{-1}$, expanding the matrices we get:

$$\left[\begin{matrix} P_{aa} & P_{ab} \\
P_{ba} & P_{bb} \\
\end{matrix}\right]
= \left[\begin{matrix} H_{aa} & H_{ab} \\
H_{ba} & H_{bb} \\
\end{matrix}\right]^{-1} =  \\
\left[\begin{matrix}I&0\\-H_{bb}^{-1}H_{ba}&I\end{matrix}\right] \left[\begin{matrix}(H_{aa}-H_{ab}H_{bb}^{-1}H_{ba})^{-1}&0\\0&H_{bb}^{-1}\end{matrix}\right]   \left[\begin{matrix}I&-H_{ab}H_{bb}^{-1}\\0&I\end{matrix}\right] $$

By writing out the top-left corner of this equation it's not difficult to find that 

$P_{aa}=(H_{aa}-H_{ab}H_{bb}^{-1}H_{ba})^{-1}$

By the symmetry of subscript $a,b$, we also have

$P_{bb}=(H_{bb}-H_{ba}H_{aa}^{-1}H_{ab})^{-1}$



Now we calculate the marginal distribution of $a$.

> $a$的期望、协方差可以直接从原来的联合分布的期望、协方差截取出来，分别是$\mu_a$和$P_{aa}$；但一般最小二乘问题中我们直接拿到的是信息矩阵，无法直接"截取"出$P_{aa}$，需要做些矩阵计算。

For symbols convenience, we first consider the case in which $\mu = x^* = 0$, then:



$$p(a,b)\propto \exp\left(-\frac{1}{2} \left[\begin{matrix} a  \\
b \\
\end{matrix}\right]
^T \left[\begin{matrix} H_{aa} & H_{ab} \\
H_{ba} & H_{bb} \\
\end{matrix}\right] \left[\begin{matrix} a  \\
b \\
\end{matrix}\right] \right)=\\
\exp\left(-\frac{1}{2} \left[\begin{matrix} a  \\
b \\
\end{matrix}\right]^T \left[\begin{matrix}I&H_{ab}H_{bb}^{-1}\\0&I\end{matrix}\right]  \left[\begin{matrix}(H_{aa}-H_{ab}H_{bb}^{-1}H_{ba})&0\\0&H_{bb}\end{matrix}\right]  \left[\begin{matrix}I&0\\H_{bb}^{-1}H_{ba}&I\end{matrix}\right] \left[\begin{matrix} a  \\
b \\
\end{matrix}\right] \right)=\\
\exp\left(-\frac{1}{2} \left[\begin{matrix} a  \\
b+H_{bb}^{-1}H_{ba}a \\
\end{matrix}\right]^T \left[\begin{matrix}(H_{aa}-H_{ab}H_{bb}^{-1}H_{ba})&0\\0&H_{bb}\end{matrix}\right]   \left[\begin{matrix} a  \\
b+H_{bb}^{-1}H_{ba}a \\
\end{matrix}\right] \right)=\\
\exp[-\frac{1}{2} a^T(H_{aa}-H_{ab}H_{bb}^{-1}H_{ba})a]  \cdot \exp[-\frac{1}{2} (b+H_{bb}^{-1}H_{ba}a)^TH_{bb}(b+H_{bb}^{-1}H_{ba}a)]\\
$$


thus we have:
$p(a)\propto \exp[-\frac{1}{2} a^T(\underbrace{H_{aa}-H_{ab}H_{bb}^{-1}H_{ba}}_{P_{aa}})a] $
$p(b|a)\propto \exp[-\frac{1}{2} (b+H_{bb}^{-1}H_{ba}a)^TH_{bb}(b+H_{bb}^{-1}H_{ba}a)]$

if $\mu = x^* \ne 0$, (marginalize $a$)

$p(a)\propto \exp[-\frac{1}{2} (a-\mu_a)^T(\underbrace{H_{aa}-H_{ab}H_{bb}^{-1}H_{ba}}_{P_{aa}})(a-\mu_a)] $
$p(b|a)\propto \exp[-\frac{1}{2} (b-\mu_b+H_{bb}^{-1}H_{ba}(a-\mu_a))^TH_{bb}(b-\mu_b+H_{bb}^{-1}H_{ba}(a-\mu_a))]$

By the symmetry of subscript $a,b$, we also have (the marginal distribution of $b$)

$p(b)\propto \exp[-\frac{1}{2} (b-\mu_b)^T(\underbrace{H_{bb}-H_{ba}H_{aa}^{-1}H_{ab}}_{P_{bb}})(b-\mu_b)] $
$p(a|b)\propto \exp[-\frac{1}{2} (a-\mu_a+H_{aa}^{-1}H_{ab}(b-\mu_b))^TH_{aa}(a-\mu_a+H_{aa}^{-1}H_{ab}(b-\mu_b))]$

> other forms
We've already shown how to calculate the marginal distribution with the information matrix $H$; If instread the covariance matrix is given, the corresponding formula is shown in the picture below:
![](https://github.com/NewThinker-Jeffrey/Figures/raw/main/figures/f943a3bf9b4c6e6ee90b35f82579a191.png)




#### Marginal likelyhood

First recall the expression for the likelyhood of a guess for $\left[ \begin{matrix} \mu_a \\ \mu_b \end{matrix} \right]$.
Let's say $\left[ \begin{matrix} x_a \\ x_b \end{matrix} \right]$ is such a guess, and $x^{E*}=\left[ \begin{matrix} a^* \\ b^* \end{matrix} \right]$ is a sample for the random variable $x^E=\left[ \begin{matrix} a \\ b \end{matrix} \right]$, then the likelyhood of the guess is:
$$L(x_a,x_b)=p(a=a^*,b=b^*)|_{\text {if } \mu_a=x_a, \mu_b=x_b} = \\
p(b^*)p(a^*|b=b^*)|_{\text {if } \mu_a=x_a, \mu_b=x_b}\propto \\
\exp[-\frac{1}{2} (b^*-x_b)^T(H_{bb}-H_{ba}H_{aa}^{-1}H_{ab})(b^*-x_b)] \cdot \exp[-\frac{1}{2} (a^*-x_a+H_{aa}^{-1}H_{ab}(b^*-x_b))^TH_{aa}(a^*-x_a+H_{aa}^{-1}H_{ab}(b^*-x_b))] \\
$$.


Now, with the prior distritubion of $b$, i.e. the marginal $p(b)$, we can also write the expression for the likelyhood of a guess for $\mu_b $ solely.
$$L(x_b)=p(b=b^*)|_{\text {if } \mu_b=x_b} = \\
p(b^*)|_{\text {if } \mu_b=x_b}\propto \\
\exp[-\frac{1}{2} (b^*-x_b)^T(H_{bb}-H_{ba}H_{aa}^{-1}H_{ab})(b^*-x_b)] \\
$$.

$L(x_b)$, The likelyhood of a guess for $\mu_b$, can be also understood as the likelyhood of a full guess for $(x_a, x_b)$, i.e. $L(x_a, x_b)$, where $x_a$ is selected to maximize $L(x_a,x_b)$ considering $x_b$ fixed. Next we show that maximizing $L(x_b)$ is actually maximizing $L(x_a,x_b)$.



Considered $L(x_a, x_b) = L^{\text{cond}}(x_a)L(x_b)$, 

where $L^{\text{cond}}(x_a)=p(a^*|b=b^*)|_{\text {if } \mu_a=x_a, \mu_b=x_b}=C\cdot \exp[-\frac{1}{2} (a^*-x_a+H_{aa}^{-1}H_{ab}(b^*-x_b))^TH_{aa}(a^*-x_a+H_{aa}^{-1}H_{ab}(b^*-x_b))] $,

where $C$ is a constant depending on $H$, it's always true that $L(x_b) \ge L(x_a, x_b)/C$;

But also note that for any $x_b$, there always exists an $x_a=a^*+H_{aa}^{-1}H_{ab}(b^*-x_b)$, such that $L(x_b) = L(x_a, x_b)/C$. If $x_b$ is fixed, this $x_a$ maximizes $L(x_a, x_b)$.

So, to maximize $L(x_a,x_b)$, we need only to choose an $x_b$ that maximizes $L(x_b)$. When such a $x_b$ is found, then we choose $x_a=a^*+H_{aa}^{-1}H_{ab}(b^*-x_b)$, and such a pair $(x_a, x_b)$ maximizes $L(x_a,x_b)$；





#### how to ?

通常在工程上，边缘化的作用在于从滑窗中移除一些时间上较老的状态量和与之关联的观测量，以确保计算量有界，而又尽可能少地损失信息（之前已说明，如果误差函数是严格的仿射函数且噪声都是高斯的，则不损失信息，这时可以理想地进行"增量式"MLE或LSE）。
比如，假设$a$对应时间比较老的状态量，我们准备将这部分状态量以及相关的观测量$y^*$移除掉。
首先再梳理一下符号：
- $t$: 代指本次边缘化发生的时刻
- $x^*$: 状态量的真值（真值是不可知的）
- $x^*_a, x^*_b$: 分别代表状态量的真值 $x^*$的$a,b$分量
- $\mu_a, \mu_b$: 分别是 $x^*_a, x^*_b$的别名，$\mu_a=x^*_a$, $\mu_b=x^*_b$；
- $x^{E*}$: $t$时刻通过 MLE 方法对$x^*$做出的估计；它是一个常量，但在 t 时刻的实验之后才可知；
- $x^E$: 在获知$x^{E*}$前对$x^{E*}$的推测，这是一个随机变量；反过来看，$x^{E*}$可看做是对随机变量$x^E$的一次采样；
- $a^*, b^*$: 分别代表常量$x^{E*}$的$a,b$分量：$a^*=x^{E*}_a$，$b^*=x^{E*}_b$ 
- $a, b$: 分别代表随机变量$x^E$的$a,b$分量：$a=x^{E}_a$，$b=x^{E}_b$
- $p(x^E)=p(a,b)$: 随机变量 $a,b$的联合概率分布
- $p(b)$: 随机变量 $b$的边缘概率分布

形式上，后续时刻中可以把 $x^E$ 看做受真实状态量 $\mu=x^*$ 控制的一个“伪”待观测量，而t时刻得到的$x^{E*}$则是它的一次采样值。
之前已说明，如果将观测量$y^*$移除的同时把"伪观测量"$x^{E*}$加进来，得到的新的MLE问题与不移除$y^*$时的原问题是等价的；
所以，在$t+1$时刻，我们可以直接从原MLE问题中删去$y^*$相关的 likelyhood 分量，而加入 $x^{E*}$ 带来的 likelyhood 分量 $L_{\text{pseudo}}(x_a, x_b)$；此时总的 likelyhood 就是
$$L(x_a,x_b,x_{\text{other}})=L_{\text{pseudo}}(x_a, x_b)L_{\text{other}}(x_b, x_{\text{other}})$$,
其中$L_{\text{other}}$是$t+1$时刻新增的观测$y_{\text{other}}$带来的likelyhood，$x_{\text{other}}$ 是新增的状态变量。
> 准确的说，$L_{\text{other}}$ 是边缘化时未被移除的观测量$y_{\text{other}}$带来的likelyhood，$x_{\text{other}}$ 是边缘化时与被移除的观测量无关的状态分量；
> t+1时刻的完整观测量可视为  $y=\left[\begin{matrix}x^E&y_{\text{other}}\end{matrix}\right]^T$

分离出 $b$ 的边缘概率分布$p(b)$后，可以把$L_{\text{pseudo}}(x_a, x_b)$拆成两项相乘：（$p(b)$相当于给出了对状态量$x_b^*$的一个先验估计）
$$L(x_a,x_b,x_{\text{other}})=L_{\text{pseudo}}^{\text{cond}}(x_a)L_{\text{pseudo}}(x_b)L_{\text{other}}(x_b, x_{\text{other}})$$
如之前所说，$L_{\text{pseudo}}^{\text{cond}}(x_a)\le C$，且无论$x_b$取到什么值，总可以导出一个$x_a$使得$L_{\text{pseudo}}^{\text{cond}}(x_a)= C$；所以，要最大化 $L(x_a,x_b,x_{\text{other}})$ 只需要最大化
$$L(x_b,x_{\text{other}})=L_{\text{pseudo}}(x_b)L_{\text{other}}(x_b, x_{\text{other}})$$
这意味着，在t+1时刻的实际计算中，我们完全不用管$x_a$，只需关注$x_b$；

由于t+1时刻$x^E$中的$a$分量完全未起作用，相当于伪观测量只有 $b$。因此，形式上，可以把 $b$ 看做受真实状态量 $\mu_b=x^*_b$ 控制的一个“伪”待观测量，而t时刻得到的$b^*$则是它的一次采样值。t+1时刻的完整观测量可视为  $y=\left[\begin{matrix}b&y_{\text{other}}\end{matrix}\right]^T$。
其中，$L_{\text{pseudo}}(x_b)$由边缘分布$p(b)$结合$t$时刻的“伪”观测值$b^*$给出；这里$L_{\text{pseudo}}(x_b)$ 应该理解为：当真实状态值$\mu_b$正好是$x_b$时，我们在$t$时刻通过MLE得到$b^*$这个值的概率。

> 总结思路就是：
不移除老的观测 
$\Leftrightarrow$ 
移除老的观测并添加伪观测$x^E$  
$\Leftrightarrow$  
只添加伪观测$x^E$中的$b$分量

可以看到，在计算$L_{\text{pseudo}}(x_b)$时，我们只需用到 $b^*$，而完全用不到 $a^*$，因此为了节约计算量，我们$t$时刻计算$x^{E*}$时，可以用舒尔补只计算出它的 $b^*$ 分量；在 t 时刻将 $a$ 对应的状态量 边缘化掉后，从 t+1 时刻开始，我们不再对 $\mu_a=x^*_a$ 做显式估计，而只更新对剩下的状态量 $\mu_b=x^*_b$ 的估计；

总结一下，边缘化操作如下：
- t时刻，我们首先用上一节的舒尔补消元方法将$t$时刻的$b^*$计算出来；$a^*$ 我们不做计算，而且之后$t$时刻之后也不再需要用它的值；
- 后续时刻，我们用公式 $L_{\text{pseudo}}(x_b)$ 来衡量对 $\mu_b=x^*_b$ 的某个guess $x_b$ 的(pseudo) likelyhood.

另一点需要注意，t时刻边缘化之后，边缘化时所涉及的观测量$y$（或error terms），在后续帧到来中都不要再使用。因为它们的作用已经转化为了先验概率$p(b)$或先验likelyhood $L_{\text{pseudo}}(x_b)$ ，后续时刻如果在使用了先验信息$p(b)$的基础上又把这些观测量考虑了一次，相当于这部分观测量被重复使用了。所以，边缘化时，能对即将被移除的状态量$a$产生约束的观测$y_{a}$，在后续帧到来中都不要再使用。从图优化的角度，$a$代表了所有被边缘化的节点，而所有与这些节点相连的边要从图中移除。这些被移除的节点和边一起又被转化成了一个先验约束$L_{\text{pseudo}}(x_b)$添加到了图中。



此外, b对应的状态变量虽然在 t 时刻后保留了下来，但后续时刻中其线性化点$s_0^b$ 要保持与t时刻一致。否则先验约束的作用就产生"畸变"了: 先验约束作用在 t 时刻的$s_0^b$的切空间上（边缘化过程中被移除的观测量相对于 b 的 Jacobian 矩阵，参考的是这个切空间上的坐标），而当切点 (线性化点)改变，在新的切空间上，$L_{\text{pseudo}}(x_b)$中涉及的协方差等参数就不能直接使用了；

（如果状态空间是欧氏空间，即便各点的切空间在几何上可以等同（可以用统一的坐标描述），但对观测模型来说它们依然不等同，因为观测函数在各点的切映射对应的 Jacobian 矩阵不同；实际上，在流形上局部选定一个 chart 后，局部内各点也有统一的坐标，各点的切空间也能在坐标意义上等同，各点的切映射也可以表达为一个Jacobian 矩阵，但不代表观测函数在点1处的Jacobian 矩阵可以直接用于点2，因为观测函数在点2一般会有不同的Jacobian 矩阵)；



