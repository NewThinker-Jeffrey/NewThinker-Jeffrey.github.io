
[TOC]

# 等式约束优化问题的求解

## 原问题描述
原问题：
$$ 问题0(Pr0)\\
objective:\qquad \min_w f(w)    \\
s.t.\qquad  h_j(w)=0;for\ j=1,2,3...m
$$

## 惩罚函数法

设计惩罚函数如下，令
$$ P_{\sigma}(w) = f(w) + \frac{\sigma}{2} \sum_{j=1}^{m}{h_j^2(w)},\qquad \sigma > 0  \tag{1.1}$$

**Inference 1** : $\sigma$充分大(趋于$+\infty$)时，下面的无约束最优化问题与原问题等价。
$$ 问题1(Pr1)\\
min_w\qquad P_{\sigma}(w) \\
\sigma \to +\infty
$$
>  **Interpretation**
$\sigma \to +\infty$时，惩罚项$\frac{\sigma}{2}\sum{h_j^2(w)}$的作用越来越显著，对于不满足约束条件的$w$，惩罚项的值将迅速增大至无穷，这样的$w$不可能是惩罚函数的最小值；只有满足约束条件的$w$才能使惩罚函数的值有限，因此惩罚函数的最小值就是在满足约束条件的前提下$f(w)$的最小值。
注意这里只是为了直观的说明两个问题的等价性，但这不是严格的证明。严格证明需要构造一个不断增加的${\sigma^k}$序列，然后分析序列的性质再取极限才能得出此结论。

利用**Inference 1**，可以想到一种求解原问题最优解的思路，就是构造一个$\sigma$充分大的惩罚函数$ P_{\sigma}(w)$，然后用无约束最优化的方法求$P_{\sigma}(w)$的最小值。因为实际计算中不可能将$\sigma$取$+\infty$，所以这样得到的最优解会稍微偏离约束条件，偏离的程度取决于$\sigma$的大小，$\sigma$越大，约束偏离越小。
实际运用中，具体该采用多大的$\sigma$是很难事先确定的。实践中一般是使用一个不断增长的$\{\sigma^k\}$序列，一般取$(\sigma^{k+1}=c\sigma^k,其中1<c<10)$，来构造一系列不同的$P_{\sigma}$，分别求解对应的$\{w^k\}$，并以$w^k$作为第$k+1$次迭代时的初值。迭代过程中，如果发现$w^k$基本不再变化，且$h(w^k)$约等于0(在允许误差范围内)就可以停止迭代了。这样就找到了足够大的$\sigma$而又不至于取到无穷。
然而，当想获得比较小的约束偏离时，一般都需要非常大的$\sigma$，一方面这会使得惩罚函数的二阶导数Hessian矩阵变得病态（各特征值的比值不均衡，数值稳定性差），另一方面计算机的浮点有效位数也面临挑战，这都为计算机数值求解带来困难。

## 增广拉格朗日乘子法
惩罚函数法的弊端在于，要想获得足够的约束吻合度，需要非常大的$\sigma$，破坏了数值稳定性。那有没有什么方法可以减少对$\sigma$的数值要求从而提高数值稳定性呢？
**增广拉格朗日乘子法**对这方面做了很好的改善，此方法中也会出现惩罚项，是对惩罚函数法的一种改进。利用**增广拉格朗日乘子法**，惩罚项的系数$\sigma$只要大于某个特定的阈值，即便不趋于无穷，也可以获得足够精确的约束吻合度。而这个阈值，要比单纯使用惩罚函数法时需要的$\sigma$小非常非常多，基本不会再破坏数值稳定性。

**Inference 2**: 对任意的$\sigma>0$时，下面问题与原问题等价。
$$ 问题2(Pr2)\\
min_w\qquad P_{\sigma}(w),\quad \sigma > 0 \\
s.t.\qquad  h_j(w)=0;for\ j=1,2,3...m
$$
> **Proof**
当约束被满足时，$P_{\sigma}(w) = f(w)$。两个问题等价。原问题的最优解和局部最优解也是此问题的最优解和局部最优解

### KKT 条件
原问题**Pr0**的拉格朗日乘子式为：
$$ L(w,\beta) = f(w) + \sum_{j=1}^{m}{\beta_i h_j(m)}  \tag{1.2}$$
等价问题**Pr2**的拉格朗日乘子式为
$$ L^p(w,\beta) =  P_{\sigma}(w) + \sum_{j=1}^{m}{\beta_i h_j(m)}  \tag{1.3}$$
如果$(w^*,\beta^*)$是满足**Pr0问题**KKT条件的一个最优解（或局部最优解），则$(w^*,\beta^*)$同样是满足**Pr2问题**KKT条件的一个最优解（或局部最优解）。反过来也一样。
> **Pr2问题**的KKT条件为
$$ \nabla f(w) + [\beta + \sigma h(w)]\nabla h(w) = 0\\
h(w) = 0 
$$
**Pr0问题**的KKT条件为
$$ \nabla f(w) + \beta \nabla h(w) = 0\\
h(w) = 0 
$$
如果$(w^*,\beta^*)$满足**Pr0问题**的KKT条件，说明：
$$ \nabla f(w^*) + \beta^*\nabla h(w^*) = 0\\
h(w^*)=0$$
此时必有
$$ \nabla f(w^*) + \beta^*\nabla h(w^*) + \sigma h(w^*)\nabla h(w^*) = 0$$
因此$(w^*,\beta^*)$同样满足**Pr2问题**的KKT条件。vice versa 。


**Inference 3**: 对任意的$\sigma>0$，下面的问题**Pr3**的最优解必是问题**Pr4**的最优解。
$$ 问题3(Pr3)\\
min_w\qquad L^p(w,\beta) =  P_{\sigma}(w) + \sum_{j=1}^{m}{\beta_i h_j(m)}\quad \sigma > 0 \\
$$
设$w^k$是问题3(Pr3)的最优解，定义问题4如下：
$$ 问题4(Pr4)\\
objective:\qquad \min_w P_{\sigma}(w)   \\
s.t.\qquad  h_j(w)=h_j(w^k);for\ j=1,2,3...m
$$
> **Proof**
$w^k$是问题3(Pr3)的最优解，因此对于任意的$w$:
$$ P_{\sigma}(w) + \sum_{j=1}^{m}{\beta_i h_j(m)} \\
\ge P_{\sigma}(w^k) + \sum_{j=1}^{m}{\beta_i h_j(w^k)} $$
如果$w$满足问题Pr4的约束，即 $h_j(w)=h_j(w^k)$，则上面的不等式变为 $P_{\sigma}(w) \ge P_{\sigma}(w^k)$，即$P_{\sigma}(w^k)$同样是问题Pr4的最小值（在满足约束的条件下）。进一步，当$h_j(w^k)=0$时，问题Pr4与问题Pr2等价。

根据**Inference 3**，我们可以先选定一个$\beta$，如$\beta=\beta^k$，然后通过求解无约束问题Pr3，来得到Pr4的一组解，记作$w^k$；如果$h(w^k)=0$或者误差可以接受，就可以直接认为$w^k$是原问题Pr0的解；否则，重新调整$\beta$，选择适当的$\beta=\beta^{k+1}$继续迭代。迭代的难点就在于怎么选择$\beta^{k+1}$，能使得$h(w^{k+1})$更接近0。
根据前面提到的KKT条件，我们可以找到一个启发式的方法。
问题Pr4的KKT条件要求：
$$ \nabla f(w) + [\beta + \sigma h(w)]\nabla h(w) = 0$$
而原问题Pr0的KKT条件要求
$$ \nabla f(w) + \beta\nabla h(w) = 0 $$ 
两个式子中$\nabla h(w)$ 的系数分别为$ [\beta + \sigma h(w)]$ 和$\beta$。当$h(w^k)=0$时，两个问题等价，上面两个式子也等价。根据$(w^k,\beta^k)$的定义，它们满足Pr4的KKT条件，即
$$ \nabla f(w^k) + [\beta + \sigma h(w^k)]\nabla h(w^k) = 0$$
为了使乘子$\beta$向原问题Pr0的最优解靠近，可选取 
$$\beta^{k+1}=\beta^{k}+\sigma h(w^k) \tag{1.4}$$
不断迭代直到$h(w^k)=0$收敛至Pr0的最优解。
然而，当$\sigma$取定某一数值时，仅依赖上述启发式迭代法来迭代 $\beta$，并不一定能收敛到最优解。事实上到现在为止，我们的论述中还没有任何证据能表明这种方式会向最优解收敛，只是因为KKT条件给我们的直觉以及当 $\beta^{k+1}=\beta^{k}+\sigma h(w^k)$ 收敛之后$h(w^k)$必变为0的事实，让我们选择了这种迭代方式。
**为了确保收敛，我们还要在调整$\beta$的同时，把$\sigma$也不断增大来加速收敛。**
同时调整$\sigma$和$\beta$，二者相配合可以使得一系列相应的无约束优化问题Pr3更快地向原问题Pr0收敛。而且有了$\beta$的存在，当$\sigma$大于某个阈值后（这个阈值一般不会大到影响数值稳定性），即便不再增加$\sigma$而只调整$\beta$，也能保证收敛。
### 增广乘子法步骤
1. 选择初始值: $\beta^1, \sigma^1, k=1$
2. 将$\beta=\beta^k,\sigma=\sigma^k$代入问题Pr3，求解问题Pr3的无约束最优解。设$w^k$是所求的最优解。
3. 如果$|h(w^k)|<\epsilon$，则停止迭代，$w^k$即是原问题的一个最优解（或局部最优解）；否则继续步骤4。其中$\epsilon\ge 0$是允许的约束偏差门限。
4. 更新 $\sigma$。若$\frac {|h(x^k)|}{|h(x^{k-1})|} > s$，说明收敛较慢，则增大$\sigma$以加快收敛，令 $\sigma^{k+1}=c\sigma^k$；否则，暂保持$\sigma$不变， $\sigma^{k+1}=\sigma^k$。其中$s$是判别收敛快慢的门限，$0<s<1$，一般选 $s=0.5$；c是$\sigma$的增长指数，一般取$1\le c \le 10$。
5. 更新 $\beta$。按照式(1.4)更新： $\beta^{k+1}=\beta^{k}+ch(w^k)$
6. k++，返回2。
> 初值$\beta^1, \sigma^1$的选取要保证问题Pr3存在最小值。

## 增广乘子法的收敛性分析
**Inference 4** 对于任意有界的$\beta$，当$\sigma$趋于无穷时，问题Pr3等价于原问题Pr0。
>**Interpretation **
分析过程与**Inference 1**类似。严格的证明过程见  Dimitri P. Bertsekas 编著的《Constrained Optimization and Lagrange Multiplier Methods》一书中的 Proposition 2.1 和 Proposition 2.2。Proposition 2.1 证明了全局最优解的等价性，Proposition 2.2证明了局部最优解的等价性。
该书网址： 
http://web.mit.edu/dimitrib/www/Constrained-Opt.pdf

有了**Inference 4**的支撑，即便我们对前面提到的按 $\beta^{k+1}=\beta^{k}+ch(w^k)$ 来更新$\beta$的方式没有把握，我们也可以确定按照上述的增广乘子法步骤进行的话，随着$\sigma$不断增大，我们也在逐步靠近最优解。特别的，如果我们保持 $\beta=0$，这就变成了原始的惩罚函数法。$\sigma$的不断增大可以保证理论上的收敛，但是会破坏数值稳定性。
下面说明为什么按 $\beta^{k+1}=\beta^{k}+ch(w^k)$ 来更新$\beta$可以辅助收敛，以及前面所说的“足够大的$\sigma$”到底需要多大。

### 对偶问题
我们首先来研究问题Pr2的对偶问题。注意Pr2与原问题Pr0等价。
定义：
$$
\theta(w) = \max_{\beta}L^p(w,\beta)\\ 
\theta_D(\beta)=\min_w L^p(w,\beta)\\
$$
则对偶问题就是求 $max_{\beta}\theta_D(\beta)$。
虽然对偶问题与问题Pr2并不一定等价，但我们不妨先试试求解对偶问题会有什么发现。
对偶问题的分析对象是函数$\theta_D(\beta)$，我们要找到能最大化$\theta_D$的$\beta^*$。为了求解对偶问题，我们可以采用梯度上升法。步骤如下：
1. 首先，选择一个合理的初值 $\beta^1, k=1$；
2. 求 $\theta_D(\beta^k)=\min_w L^p(w,\beta^k)$ 。其实主要是求出能最小化 $L^p(w,\beta^k)$ 的$w$值，表示为$w^k$；
3. 求出 $\nabla \theta_D(\beta^k)$，经分析可知 $\nabla \theta_D(\beta^k)=h(w^k)$（后面有证明）；
4. 如果$\nabla \theta_D(\beta^k)=h(w^k)=0$或在误差允许范围内，则迭代结束；
否则，沿梯度方向更新$\beta$：
$\beta^{k+1} = \beta^k + \lambda \nabla \theta_D(\beta^k) = \beta^k + \lambda h(w^k) $
其中$\lambda>0$。如果取 $\lambda=\sigma$，则有
$\beta^{k+1} = \beta^k + \sigma h(w^k) $
这跟式(1.4)是完全一样的，此时梯度上升的步长为 $|\sigma h(w^k)|$
5. k++, 跳回2。

> **局部极值点及鞍点问题**
上述梯度上升法更新$\beta$，可能收敛到局部极值点或鞍点，甚至如果固定步长$|\sigma h(w^k)|$不合理时会不收敛；另外，第2步最小化$L^p(w,\beta^k)$ 时如果也是用梯度下降法，在实际中更可能得到的也只是一个梯度为0的局部极小值点甚至是鞍点，为了准确表述我们后面只称这个点为**解点**（如果此步求得的解点不是真正的最小点，我们得到的对应的$L^p(w,\beta^k)$其实就已经不再等于真正的$\theta_D(\beta^k)$了）；但即便这些都发生了，我们按照这种迭代方式（如果$\beta$能收敛）得出的结果也一定是满足原问题的KKT条件的。因为步骤2得到的$w$一定满足$\nabla_w L^p(w,\beta^k)=0$，$\beta$梯度收敛后又必有 $\nabla \theta_D(\beta) = h(w)=0 $，因此即使找不到全局最优解，也能找到一组满足KKT条件的解。唯一还需要捏把汗的地方，是我们盲目地选择了梯度上升的步长$|\sigma h(w^k)|$。
（更详细的对收敛性进行论证的步骤参见书中的第二章，这本书非常难啃下来，里面的证明很繁杂，但讲解最优化方法最为全面了）

> **关于第3步中$\nabla \theta_D(\beta^k)=h(w^k)$的证明**
假如$\beta$由$\beta^k$变为$\beta^k+\Delta \beta$，对应的$L^p(w,\beta)$的**解点**$w$由$w^k$变为$w^k+\Delta w$，此时

$$
\Delta \theta_D = \Delta P_{\sigma}(w)  + \Delta [\beta h(w)] \\
= \nabla P_{\sigma}(w^k) \cdot \Delta w + [\beta^k \nabla h(w)  \cdot \Delta w + h(w^k)\cdot \Delta \beta ] \\
= [\nabla P_{\sigma}(w^k) + \beta^k \nabla h(w)]  \cdot \Delta w + h(w^k)\cdot \Delta \beta \\
= \nabla_w L^p(w^k, \beta^k) \cdot \Delta w+ h(w^k)\cdot \Delta \beta$$

而由于 $w^k$ 是$L^p(w, \beta^k)$的一个**解点**，即$\nabla_w L^p(w^k, \beta^k)=0$，于是：
$$ \Delta \theta_D = h(w^k)\cdot \Delta \beta $$
因此 $\nabla \theta_D(\beta^k) = h(w^k) $

上面的第4步中，出现了与之前式(1.4)完全一样的$\beta$迭代式。这说明，按照(1.4)式更新的$\beta$并不是毫无意义，At least，这会逐步靠近对偶问题的解。其实，求解对偶问题的步骤与前述增广乘子法的完整步骤几乎一样，只是没有对 $\sigma$进行更新。如果增广乘子法中不更新$\sigma$的话，那就完全是在求解对偶问题！

